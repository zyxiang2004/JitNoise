{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d20d6203-b6f6-427f-8d5d-28086d6f615a",
   "metadata": {},
   "source": [
    "# pytorch版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df31da09-8057-40b3-a41c-399be0da1c11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:08<00:00, 45.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.124706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:08<00:00, 44.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.062822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:08<00:00, 45.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.055694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:08<00:00, 44.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.052523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:08<00:00, 44.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.049559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:08<00:00, 45.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 0.047112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:08<00:00, 44.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 0.045910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:08<00:00, 44.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 0.044645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:08<00:00, 45.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 0.043990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:08<00:00, 45.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.043917\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "# 设置设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# UNet 模型定义\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3):\n",
    "        super(UNet, self).__init__()\n",
    "        def double_conv(in_ch, out_ch):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        \n",
    "        self.down1 = double_conv(in_channels, 64)\n",
    "        self.down2 = double_conv(64, 128)\n",
    "        self.down3 = double_conv(128, 256)\n",
    "        self.up1 = double_conv(256 + 128, 128)\n",
    "        self.up2 = double_conv(128 + 64, 64)\n",
    "        self.up3 = nn.Conv2d(64, out_channels, 1)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.time_emb = nn.Sequential(\n",
    "            nn.Linear(1, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t_emb = self.time_emb(t.view(-1, 1)).view(-1, 64, 1, 1)\n",
    "        x1 = self.down1(x)\n",
    "        x2 = self.pool(x1)\n",
    "        x2 = self.down2(x2)\n",
    "        x3 = self.pool(x2)\n",
    "        x3 = self.down3(x3)\n",
    "        x = self.upsample(x3)\n",
    "        x = torch.cat([x, x2], dim=1)\n",
    "        x = self.up1(x)\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, x1], dim=1)\n",
    "        x = self.up2(x)\n",
    "        x = self.up3(x)\n",
    "        return x\n",
    "\n",
    "# 扩散模型类\n",
    "class DiffusionModel:\n",
    "    def __init__(self, T=1000, beta_start=1e-4, beta_end=0.02):\n",
    "        self.T = T\n",
    "        self.beta = torch.linspace(beta_start, beta_end, T).to(device)\n",
    "        self.alpha = 1.0 - self.beta\n",
    "        self.alpha_bar = torch.cumprod(self.alpha, dim=0)\n",
    "    \n",
    "    def forward_process(self, x0, t, noise=None):\n",
    "        \"\"\"正向过程：添加噪声\"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x0).to(device)\n",
    "        sqrt_alpha_bar = torch.sqrt(self.alpha_bar[t]).view(-1, 1, 1, 1)\n",
    "        sqrt_one_minus_alpha_bar = torch.sqrt(1 - self.alpha_bar[t]).view(-1, 1, 1, 1)\n",
    "        xt = sqrt_alpha_bar * x0 + sqrt_one_minus_alpha_bar * noise\n",
    "        return xt, noise\n",
    "    \n",
    "    def sample_step(self, model, xt, t):\n",
    "        \"\"\"反向过程单步\"\"\"\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            beta_t = self.beta[t].view(-1, 1, 1, 1)\n",
    "            alpha_t = self.alpha[t].view(-1, 1, 1, 1)\n",
    "            alpha_bar_t = self.alpha_bar[t].view(-1, 1, 1, 1)\n",
    "            noise_pred = model(xt, t / self.T)\n",
    "            x_prev = (xt - beta_t / torch.sqrt(1 - alpha_bar_t) * noise_pred) / torch.sqrt(alpha_t)\n",
    "            if t > 0:\n",
    "                x_prev += torch.sqrt(beta_t) * torch.randn_like(xt)\n",
    "        return x_prev\n",
    "\n",
    "# 数据加载\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "# 训练设置\n",
    "model = UNet().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-4)\n",
    "diffusion = DiffusionModel(T=1000)\n",
    "num_epochs = 10\n",
    "os.makedirs('samples', exist_ok=True)\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "\n",
    "# 日志设置\n",
    "logging.basicConfig(filename='logs/training.log', level=logging.INFO, \n",
    "                    format='%(asctime)s - %(message)s')\n",
    "losses = []\n",
    "\n",
    "# 训练循环\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch_idx, (images, _) in enumerate(tqdm(trainloader)):\n",
    "        images = images.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 随机时间步\n",
    "        t = torch.randint(0, diffusion.T, (images.size(0),), device=device).float()\n",
    "        xt, noise = diffusion.forward_process(images, t.long())\n",
    "        \n",
    "        # 预测噪声\n",
    "        noise_pred = model(xt, t / diffusion.T)\n",
    "        loss = nn.MSELoss()(noise_pred, noise)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(trainloader)\n",
    "    losses.append(avg_loss)\n",
    "    logging.info(f'Epoch {epoch+1}, Loss: {avg_loss:.6f}')\n",
    "    print(f'Epoch {epoch+1}, Loss: {avg_loss:.6f}')\n",
    "    \n",
    "    # 每5个 epoch 保存采样图像\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        model.eval()\n",
    "        sample = torch.randn(16, 3, 32, 32).to(device)\n",
    "        for t in reversed(range(diffusion.T)):\n",
    "            sample = diffusion.sample_step(model, sample, torch.tensor([t]).to(device))\n",
    "        sample = (sample.clamp(-1, 1) + 1) / 2\n",
    "        grid = torchvision.utils.make_grid(sample, nrow=4)\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "        plt.axis('off')\n",
    "        plt.savefig(f'samples/epoch_{epoch+1}.png')\n",
    "        plt.close()\n",
    "\n",
    "# 绘制 loss 曲线\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.legend()\n",
    "plt.savefig('logs/loss_curve.png')\n",
    "plt.close()\n",
    "\n",
    "# 保存模型\n",
    "torch.save(model.state_dict(), 'unet_diffusion_cifar10.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17843bc-a2e3-4902-97b1-e28e8188829a",
   "metadata": {},
   "source": [
    "# jittor版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1aa0e9c6-7acd-4063-8465-d442f4687e62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[38;5;2m[i 0502 17:33:30.463300 36 log.cc:351] Load log_sync: 1\u001B[m\n",
      "\u001B[38;5;2m[i 0502 17:33:30.565032 36 compiler.py:956] Jittor(1.3.9.14) src: /root/miniconda3/lib/python3.8/site-packages/jittor\u001B[m\n",
      "\u001B[38;5;2m[i 0502 17:33:30.578478 36 compiler.py:957] g++ at /usr/bin/g++(9.4.0)\u001B[m\n",
      "\u001B[38;5;2m[i 0502 17:33:30.580292 36 compiler.py:958] cache_path: /root/.cache/jittor/jt1.3.9/g++9.4.0/py3.8.10/Linux-5.15.0-9xd1/IntelRXeonRPlaxda/480a/default\u001B[m\n",
      "\u001B[38;5;2m[i 0502 17:33:30.593694 36 __init__.py:412] Found nvcc(11.8.89) at /usr/local/cuda/bin/nvcc.\u001B[m\n",
      "\u001B[38;5;2m[i 0502 17:33:30.606103 36 __init__.py:412] Found addr2line(2.34) at /usr/bin/addr2line.\u001B[m\n",
      "\u001B[38;5;2m[i 0502 17:33:30.906809 36 compiler.py:1013] cuda key:cu11.8.89_sm_89\u001B[m\n",
      "\u001B[38;5;2m[i 0502 17:33:31.540729 36 __init__.py:227] Total mem: 1007.51GB, using 16 procs for compiling.\u001B[m\n",
      "\u001B[38;5;2m[i 0502 17:33:32.129632 36 jit_compiler.cc:28] Load cc_path: /usr/bin/g++\u001B[m\n",
      "\u001B[38;5;2m[i 0502 17:33:32.375481 36 init.cc:63] Found cuda archs: [89,]\u001B[m\n",
      "\u001B[38;5;2m[i 0502 17:33:35.465433 36 cuda_flags.cc:49] CUDA enabled.\u001B[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/391 [00:00<?, ?it/s]\u001B[38;5;3m[w 0502 17:33:37.558300 36 grad.cc:81] grads[42] 'time_emb.0.weight' doesn't have gradient. It will be set to zero: Var(638:1:1:1:i0:o0:s1:n1:g1,float32,time_emb.0.weight,7ff0ef6fdc00)[64,1,]\u001B[m\n",
      "\u001B[38;5;3m[w 0502 17:33:37.561556 36 grad.cc:81] grads[43] 'time_emb.0.bias' doesn't have gradient. It will be set to zero: Var(657:1:1:1:i0:o0:s1:n0:g1,float32,time_emb.0.bias,7ff0ef6fde00)[64,]\u001B[m\n",
      "\u001B[38;5;3m[w 0502 17:33:37.562850 36 grad.cc:81] grads[44] 'time_emb.2.weight' doesn't have gradient. It will be set to zero: Var(676:1:1:1:i0:o0:s1:n1:g1,float32,time_emb.2.weight,7ff220e52800)[64,64,]\u001B[m\n",
      "\u001B[38;5;3m[w 0502 17:33:37.563833 36 grad.cc:81] grads[45] 'time_emb.2.bias' doesn't have gradient. It will be set to zero: Var(695:1:1:1:i0:o0:s1:n0:g1,float32,time_emb.2.bias,7ff0ef6fe000)[64,]\u001B[m\n",
      "100%|██████████| 391/391 [00:12<00:00, 32.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.125921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:10<00:00, 38.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.062329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:10<00:00, 38.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.055939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:10<00:00, 38.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.051543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:10<00:00, 38.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.049637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:10<00:00, 38.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 0.048435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:10<00:00, 39.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 0.046073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:10<00:00, 38.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 0.044620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:10<00:00, 38.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 0.044917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:10<00:00, 38.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.042958\n"
     ]
    }
   ],
   "source": [
    "import jittor as jt\n",
    "from jittor import nn, optim, dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import jittor.transform as transforms\n",
    "# 设置设备\n",
    "jt.flags.use_cuda = 1  # jt.has_cuda\n",
    "jt.flags.amp_reg = 0  # 禁用自动混合精度（若未使用AMP）\n",
    "\n",
    "# UNet 模型定义\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        def double_conv(in_ch, out_ch):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv(in_ch, out_ch, 3, padding=1),\n",
    "                nn.BatchNorm(out_ch),\n",
    "                nn.ReLU(),  # CBL\n",
    "                nn.Conv(out_ch, out_ch, 3, padding=1),\n",
    "                nn.BatchNorm(out_ch),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "\n",
    "        self.down1 = double_conv(in_channels, 64)\n",
    "        self.down2 = double_conv(64, 128)\n",
    "        self.down3 = double_conv(128, 256)\n",
    "        self.up1 = double_conv(256 + 128, 128)\n",
    "        self.up2 = double_conv(128 + 64, 64)\n",
    "        self.up3 = nn.Conv(64, out_channels, 1)\n",
    "        self.pool = nn.Pool(2, op='maximum')\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.time_emb = nn.Sequential(\n",
    "            nn.Linear(1, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64)\n",
    "        )\n",
    "\n",
    "    def execute(self, x, t):\n",
    "        t_emb = self.time_emb(t.reshape(-1, 1)).reshape(-1, 64, 1, 1)\n",
    "        x1 = self.down1(x)\n",
    "        x2 = self.pool(x1)\n",
    "        x2 = self.down2(x2)\n",
    "        x3 = self.pool(x2)\n",
    "        x3 = self.down3(x3)\n",
    "        x = self.upsample(x3)\n",
    "        x = jt.concat([x, x2], dim=1)\n",
    "        x = self.up1(x)\n",
    "        x = self.upsample(x)\n",
    "        x = jt.concat([x, x1], dim=1)\n",
    "        x = self.up2(x)\n",
    "        x = self.up3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 扩散模型类\n",
    "class DiffusionModel:\n",
    "    def __init__(self, T=1000, beta_start=1e-4, beta_end=0.02):\n",
    "        self.T = T\n",
    "        self.beta = jt.linspace(beta_start, beta_end, T)\n",
    "        self.alpha = 1.0 - self.beta\n",
    "        self.alpha_bar = jt.cumprod(self.alpha, dim=0)\n",
    "\n",
    "    def forward_process(self, x0, t, noise=None):\n",
    "        \"\"\"正向过程：添加噪声\"\"\"\n",
    "        if noise is None:\n",
    "            noise = jt.randn_like(x0)\n",
    "        sqrt_alpha_bar = jt.sqrt(self.alpha_bar[t]).reshape(-1, 1, 1, 1)\n",
    "        sqrt_one_minus_alpha_bar = jt.sqrt(1 - self.alpha_bar[t]).reshape(-1, 1, 1, 1)\n",
    "        xt = sqrt_alpha_bar * x0 + sqrt_one_minus_alpha_bar * noise\n",
    "        return xt, noise\n",
    "\n",
    "    def sample_step(self, model, xt, t):\n",
    "        \"\"\"反向过程单步\"\"\"\n",
    "        model.eval()\n",
    "        with jt.no_grad():\n",
    "            beta_t = self.beta[t].reshape(-1, 1, 1, 1)\n",
    "            alpha_t = self.alpha[t].reshape(-1, 1, 1, 1)\n",
    "            alpha_bar_t = self.alpha_bar[t].reshape(-1, 1, 1, 1)\n",
    "            noise_pred = model(xt, t / self.T)\n",
    "            x_prev = (xt - beta_t / jt.sqrt(1 - alpha_bar_t) * noise_pred) / jt.sqrt(alpha_t)\n",
    "        if t > 0:\n",
    "            x_prev += jt.sqrt(beta_t) * jt.randn_like(xt)\n",
    "        jt.gc() #触发垃圾回收\n",
    "        return x_prev\n",
    "\n",
    "\n",
    "# 数据加载\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.ImageNormalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "trainset = dataset.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "batch_size = 128\n",
    "total_batches = (len(trainset) + batch_size - 1) // batch_size\n",
    "trainloader = dataset.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "# 训练设置\n",
    "model = UNet()\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-4)\n",
    "diffusion = DiffusionModel(T=1000)\n",
    "num_epochs = 10\n",
    "os.makedirs('samples', exist_ok=True)\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "\n",
    "# 日志设置\n",
    "logging.basicConfig(filename='logs/training.log', level=logging.INFO,\n",
    "                    format='%(asctime)s - %(message)s')\n",
    "losses = []\n",
    "\n",
    "\n",
    "# 保存图像的函数（Jittor 没有直接的 make_grid，需手动实现）\n",
    "def save_image_grid(images, filename, nrow=4):\n",
    "    images = (images.clamp(-1, 1) + 1) / 2\n",
    "    images = images.numpy().transpose(0, 2, 3, 1)  # (B, C, H, W) -> (B, H, W, C)\n",
    "    n = images.shape[0]\n",
    "    ncols = nrow\n",
    "    nrows = (n + ncols - 1) // ncols\n",
    "    grid = np.zeros((nrows * 32, ncols * 32, 3))\n",
    "    for i in range(n):\n",
    "        row = i // ncols\n",
    "        col = i % ncols\n",
    "        grid[row * 32:(row + 1) * 32, col * 32:(col + 1) * 32] = images[i]\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(grid)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# 训练循环\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch_idx, (images, _) in enumerate(tqdm(trainloader, total=total_batches)):\n",
    "        images = jt.array(images)\n",
    "\n",
    "        # 随机时间步\n",
    "        t = jt.randint(0, diffusion.T, (images.shape[0],)).float()\n",
    "        xt, noise = diffusion.forward_process(images, t.int())\n",
    "\n",
    "        # 预测噪声\n",
    "        noise_pred = model(xt, t / diffusion.T)\n",
    "        loss = nn.mse_loss(noise_pred, noise)\n",
    "\n",
    "        optimizer.step(loss)\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_loss = epoch_loss / total_batches\n",
    "    losses.append(avg_loss)\n",
    "    logging.info(f'Epoch {epoch + 1}, Loss: {avg_loss:.6f}')\n",
    "    print(f'Epoch {epoch + 1}, Loss: {avg_loss:.6f}')\n",
    "\n",
    "    # 每5个 epoch 保存采样图像\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        model.eval()\n",
    "        sample = jt.randn(16, 3, 32, 32)\n",
    "        with jt.no_grad():  # 添加此行，禁用梯度\n",
    "            for t in reversed(range(diffusion.T)):\n",
    "                sample = diffusion.sample_step(model, sample, jt.array([t]))\n",
    "                jt.gc() #主动垃圾回收\n",
    "        save_image_grid(sample, f'samples/epoch_{epoch + 1}.png')\n",
    "\n",
    "# 绘制 loss 曲线\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.legend()\n",
    "plt.savefig('logs/loss_curve.png')\n",
    "plt.close()\n",
    "\n",
    "# 保存模型\n",
    "jt.save(model.state_dict(), 'unet_diffusion_cifar10.jt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6527e7-518a-4501-a25b-ef6712ceab83",
   "metadata": {},
   "source": [
    "# DDPM创新版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d626adab-380a-45bf-bfe6-bd2b547d4f8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[38;5;2m[i 0503 17:13:38.702224 60 log.cc:351] Load log_sync: 1\u001B[m\n",
      "\u001B[38;5;2m[i 0503 17:13:38.761555 60 compiler.py:956] Jittor(1.3.9.14) src: /root/miniconda3/lib/python3.8/site-packages/jittor\u001B[m\n",
      "\u001B[38;5;2m[i 0503 17:13:38.771698 60 compiler.py:957] g++ at /usr/bin/g++(9.4.0)\u001B[m\n",
      "\u001B[38;5;2m[i 0503 17:13:38.772799 60 compiler.py:958] cache_path: /root/.cache/jittor/jt1.3.9/g++9.4.0/py3.8.10/Linux-5.15.0-9xc6/IntelRXeonRGolx95/480a/default\u001B[m\n",
      "\u001B[38;5;2m[i 0503 17:13:38.782032 60 __init__.py:412] Found nvcc(11.8.89) at /usr/local/cuda/bin/nvcc.\u001B[m\n",
      "\u001B[38;5;2m[i 0503 17:13:38.791479 60 __init__.py:412] Found addr2line(2.34) at /usr/bin/addr2line.\u001B[m\n",
      "\u001B[38;5;2m[i 0503 17:13:39.073639 60 compiler.py:1013] cuda key:cu11.8.89_sm_89\u001B[m\n",
      "\u001B[38;5;2m[i 0503 17:13:39.684431 60 __init__.py:227] Total mem: 1007.52GB, using 16 procs for compiling.\u001B[m\n",
      "\u001B[38;5;2m[i 0503 17:13:39.898776 60 jit_compiler.cc:28] Load cc_path: /usr/bin/g++\u001B[m\n",
      "\u001B[38;5;2m[i 0503 17:13:40.214279 60 init.cc:63] Found cuda archs: [89,]\u001B[m\n",
      "\u001B[38;5;2m[i 0503 17:13:42.524101 60 cuda_flags.cc:49] CUDA enabled.\u001B[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jittor CUDA状态: 1\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 391/391 [01:11<00:00,  5.49batch/s, Batch Loss=0.0899, Avg Loss=0.1719]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 平均损失: 0.1719\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 391/391 [01:06<00:00,  5.85batch/s, Batch Loss=0.1068, Avg Loss=0.0830]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 平均损失: 0.0830\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 391/391 [01:06<00:00,  5.86batch/s, Batch Loss=0.0698, Avg Loss=0.0720]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 平均损失: 0.0720\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 391/391 [01:06<00:00,  5.86batch/s, Batch Loss=0.0833, Avg Loss=0.0664]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 平均损失: 0.0664\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 391/391 [01:06<00:00,  5.86batch/s, Batch Loss=0.0507, Avg Loss=0.0627]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 平均损失: 0.0627\n",
      "\n",
      "已调整beta计划，当前T=900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 391/391 [01:06<00:00,  5.89batch/s, Batch Loss=0.0385, Avg Loss=0.0628]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 平均损失: 0.0628\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 391/391 [01:06<00:00,  5.85batch/s, Batch Loss=0.0734, Avg Loss=0.0601]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 平均损失: 0.0601\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 391/391 [01:07<00:00,  5.83batch/s, Batch Loss=0.0654, Avg Loss=0.0575]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 平均损失: 0.0575\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 391/391 [01:06<00:00,  5.85batch/s, Batch Loss=0.0495, Avg Loss=0.0565]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 平均损失: 0.0565\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 391/391 [01:06<00:00,  5.86batch/s, Batch Loss=0.0916, Avg Loss=0.0550]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10 平均损失: 0.0550\n",
      "\n",
      "已调整beta计划，当前T=810\n",
      "已保存模型至 unet_diffusion_cifar10.jt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 810/810 [00:02<00:00, 311.56it/s]\n"
     ]
    }
   ],
   "source": [
    "import jittor as jt\n",
    "from jittor import nn, optim, dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import jittor.transform as transforms\n",
    "from jittor.models import vgg16\n",
    "\n",
    "# 环境配置\n",
    "jt.flags.use_cuda = 1  # 启用CUDA\n",
    "jt.flags.amp_reg = 0  # 禁用自动混合精度\n",
    "print(\"Jittor CUDA状态:\", jt.flags.use_cuda)\n",
    "\n",
    "# 创建必要目录\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "os.makedirs(\"samples\", exist_ok=True)\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "# 日志配置\n",
    "logging.basicConfig(\n",
    "    filename='logs/training.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(message)s'\n",
    ")\n",
    "\n",
    "\n",
    "# ------------------------ 模型定义 ------------------------#\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, num_classes=10):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # 时间+类别嵌入\n",
    "        self.time_emb = nn.Sequential(\n",
    "            nn.Linear(1, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64)\n",
    "        )\n",
    "        self.label_emb = nn.Embedding(num_classes, 64)  # 条件嵌入\n",
    "\n",
    "        # 下采样层\n",
    "        def double_conv(in_ch, out_ch):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv(in_ch, out_ch, 3, padding=1),\n",
    "                nn.BatchNorm(out_ch),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv(out_ch, out_ch, 3, padding=1),\n",
    "                nn.BatchNorm(out_ch),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "\n",
    "        self.down1 = double_conv(in_channels, 64)\n",
    "        self.down2 = double_conv(64, 128)\n",
    "        self.down3 = double_conv(128, 256)\n",
    "\n",
    "        # 上采样层\n",
    "        self.up1 = double_conv(256 + 128, 128)\n",
    "        self.up2 = double_conv(128 + 64, 64)\n",
    "        self.up3 = nn.Conv(64, out_channels, 1)\n",
    "\n",
    "        # 辅助层\n",
    "        self.pool = nn.Pool(2, op='maximum')\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "\n",
    "    def execute(self, x, t, labels=None):\n",
    "        # 嵌入处理\n",
    "        t_emb = self.time_emb(t.reshape(-1, 1)).reshape(-1, 64, 1, 1)\n",
    "        if labels is not None:\n",
    "            label_emb = self.label_emb(labels).reshape(-1, 64, 1, 1)\n",
    "            t_emb += label_emb  # 融合条件信息\n",
    "\n",
    "        # 编码器\n",
    "        x1 = self.down1(x) + t_emb  # [B,64,H,W]\n",
    "        x2 = self.pool(x1)\n",
    "        x2 = self.down2(x2)  # [B,128,H/2,W/2]\n",
    "        x3 = self.pool(x2)\n",
    "        x3 = self.down3(x3)  # [B,256,H/4,W/4]\n",
    "\n",
    "        # 解码器\n",
    "        x = self.upsample(x3)\n",
    "        x = jt.concat([x, x2], dim=1)\n",
    "        x = self.up1(x)\n",
    "        x = self.upsample(x)\n",
    "        x = jt.concat([x, x1], dim=1)\n",
    "        x = self.up2(x)\n",
    "        return self.up3(x)\n",
    "\n",
    "\n",
    "# ----------------------- 扩散模型核心 -----------------------#\n",
    "class DiffusionModel:\n",
    "    def __init__(self, T=1000, beta_start=1e-4, beta_end=0.02):\n",
    "        self.T_initial = T\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_end = beta_end\n",
    "        self.current_T = T  # 渐进式采样\n",
    "        self.loss_sum = jt.zeros(T)\n",
    "        self.loss_count = jt.zeros(T)\n",
    "        self.update_beta_schedule()\n",
    "\n",
    "    def update_beta_schedule(self):\n",
    "        \"\"\"更新alpha和beta序列\"\"\"\n",
    "        self.beta = jt.linspace(self.beta_start, self.beta_end, self.current_T)\n",
    "        self.alpha = 1.0 - self.beta\n",
    "        self.alpha_bar = jt.cumprod(self.alpha, dim=0)\n",
    "\n",
    "    def adjust_beta(self):\n",
    "        avg_loss = (self.loss_sum + 1e-8) / (self.loss_count + 1e-8)\n",
    "        weights = avg_loss / avg_loss.sum()\n",
    "\n",
    "        # 动态调整current_T和beta\n",
    "        self.current_T = max(100, int(self.current_T * 0.9))\n",
    "        self.beta = self.beta_start + (self.beta_end - self.beta_start) * weights.cumsum()[:self.current_T]\n",
    "\n",
    "        self.update_beta_schedule()\n",
    "        self.loss_sum = jt.zeros_like(self.loss_sum)\n",
    "        self.loss_count = jt.zeros_like(self.loss_count)\n",
    "\n",
    "    def forward_process(self, x0, t, noise=None):\n",
    "        \"\"\"前向扩散过程\"\"\"\n",
    "        if noise is None:\n",
    "            noise = jt.randn_like(x0)\n",
    "        sqrt_alpha_bar = jt.sqrt(self.alpha_bar[t]).reshape(-1, 1, 1, 1)\n",
    "        sqrt_one_minus_alpha_bar = jt.sqrt(1 - self.alpha_bar[t]).reshape(-1, 1, 1, 1)\n",
    "        xt = sqrt_alpha_bar * x0 + sqrt_one_minus_alpha_bar * noise\n",
    "        return xt, noise\n",
    "\n",
    "    def sample_step(self, model, xt, t, labels=None):\n",
    "        \"\"\"单步采样\"\"\"\n",
    "        model.eval()\n",
    "        with jt.no_grad():\n",
    "            beta_t = self.beta[t].reshape(-1, 1, 1, 1)\n",
    "            alpha_t = self.alpha[t].reshape(-1, 1, 1, 1)\n",
    "            alpha_bar_t = self.alpha_bar[t].reshape(-1, 1, 1, 1)\n",
    "\n",
    "            # 条件生成支持\n",
    "            noise_pred = model(xt, t / self.T_initial, labels=labels)\n",
    "\n",
    "            # 逆向过程计算\n",
    "            x_prev = (xt - beta_t / jt.sqrt(1 - alpha_bar_t) * noise_pred) / jt.sqrt(alpha_t)\n",
    "            if t > 0:\n",
    "                x_prev += jt.sqrt(beta_t) * jt.randn_like(xt)\n",
    "            return x_prev\n",
    "\n",
    "\n",
    "# --------------------- 感知损失模块 ---------------------#\n",
    "class VGGPerceptualLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        vgg = vgg16(pretrained=True).features\n",
    "        self.slice = nn.Sequential()\n",
    "        for i in range(9):  # 使用前4个卷积层\n",
    "            self.slice.add_module(str(i), vgg[i])\n",
    "        self.eval()\n",
    "\n",
    "    def execute(self, pred, target):\n",
    "        pred = (pred + 1) / 2  # 归一化到[0,1]\n",
    "        target = (target + 1) / 2\n",
    "        pred = nn.interpolate(pred, (224, 224), mode='bilinear')\n",
    "        target = nn.interpolate(target, (224, 224), mode='bilinear')\n",
    "        return nn.mse_loss(self.slice(pred), self.slice(target))\n",
    "\n",
    "\n",
    "# --------------------- 数据加载 ----------------------#\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.ImageNormalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "trainset = dataset.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "batch_size = 128\n",
    "trainloader = dataset.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "total_batches = (len(trainset) + batch_size - 1) // batch_size\n",
    "\n",
    "# --------------------- 训练初始化 ---------------------#\n",
    "model = UNet(num_classes=10)\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-4)\n",
    "diffusion = DiffusionModel(T=1000)\n",
    "vgg_loss = VGGPerceptualLoss()\n",
    "\n",
    "\n",
    "# --------------------- 条件采样函数 ----------------------#\n",
    "def conditional_sample(model, diffusion, labels, num_samples=16):\n",
    "    x = jt.randn(num_samples, 3, 32, 32)\n",
    "    indices = list(range(diffusion.current_T))[::-1]\n",
    "\n",
    "    for t in tqdm(indices, desc=\"Sampling\"):\n",
    "        x = diffusion.sample_step(model, x, jt.array([t]), labels=labels)\n",
    "        x = x.clamp(-1, 1)\n",
    "    return x\n",
    "\n",
    "\n",
    "# --------------------- 辅助函数 ----------------------#\n",
    "def save_image_grid(images, filename, nrow=4):\n",
    "    \"\"\" 保存图像网格 \"\"\"\n",
    "    n = images.shape[0]\n",
    "    ncols = min(nrow, n)\n",
    "    nrows = (n + ncols - 1) // ncols\n",
    "\n",
    "    grid = np.zeros((nrows * 32, ncols * 32, 3))\n",
    "    for i in range(n):\n",
    "        row = i // ncols\n",
    "        col = i % ncols\n",
    "        grid[row * 32:(row + 1) * 32, col * 32:(col + 1) * 32] = images[i]\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(grid)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(filename, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# --------------------- 训练循环 ----------------------#\n",
    "num_epochs = 10\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    with tqdm(total=total_batches, desc=f\"Epoch {epoch + 1}/{num_epochs}\", unit='batch') as pbar:\n",
    "        for batch_idx, (images, labels) in enumerate(trainloader):\n",
    "            images = jt.array(images)\n",
    "            labels = jt.array(labels)\n",
    "\n",
    "            # 扩散过程\n",
    "            t = jt.randint(0, diffusion.current_T, (images.shape[0],)).float()\n",
    "            xt, noise = diffusion.forward_process(images, t.int())\n",
    "\n",
    "            # 前向计算\n",
    "            noise_pred = model(xt, t / diffusion.T_initial, labels)\n",
    "            loss_mse = nn.mse_loss(noise_pred, noise)\n",
    "            loss_perceptual = vgg_loss(noise_pred, noise)\n",
    "            loss = loss_mse + 0.05 * loss_perceptual\n",
    "\n",
    "            # 反向传播\n",
    "            optimizer.step(loss)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # 更新进度条\n",
    "            pbar.set_postfix({\n",
    "                \"Batch Loss\": f\"{loss.item():.4f}\",\n",
    "                \"Avg Loss\": f\"{epoch_loss / (batch_idx + 1):.4f}\"\n",
    "            })\n",
    "            pbar.update(1)\n",
    "\n",
    "            # 记录时间步损失\n",
    "            loss_per_sample = (noise_pred - noise).sqr().mean(dims=[1, 2, 3])\n",
    "            for ti, li in zip(t.numpy(), loss_per_sample.numpy()):\n",
    "                diffusion.loss_sum[ti] += li\n",
    "                diffusion.loss_count[ti] += 1\n",
    "\n",
    "            # 内存管理\n",
    "            if batch_idx % 100 == 0:\n",
    "                jt.gc()\n",
    "                jt.sync_all()\n",
    "\n",
    "    # 记录epoch损失\n",
    "    avg_loss = epoch_loss / total_batches\n",
    "    loss_history.append(avg_loss)\n",
    "    logging.info(f\"Epoch {epoch + 1} Average Loss: {avg_loss:.4f}\")\n",
    "    print(f\"\\nEpoch {epoch + 1} 平均损失: {avg_loss:.4f}\\n\")\n",
    "\n",
    "    # 动态调整beta计划\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        diffusion.adjust_beta()\n",
    "        print(f\"已调整beta计划，当前T={diffusion.current_T}\")\n",
    "\n",
    "    # 保存模型和生成样本\n",
    "    if epoch == num_epochs - 1:  # 最后一个epoch保存\n",
    "        save_path = f\"unet_diffusion_cifar10.jt\"  # 最外层路径\n",
    "        jt.save(model.state_dict(), save_path)\n",
    "        print(f\"已保存模型至 {save_path}\")\n",
    "\n",
    "        # 条件采样示例\n",
    "        sample_labels = jt.randint(0, 10, (16,))\n",
    "        samples = conditional_sample(model, diffusion, sample_labels, num_samples=16)\n",
    "        samples = (samples.clamp(-1, 1) + 1) / 2  # 转换为[0,1]\n",
    "        samples_np = samples.numpy().transpose(0, 2, 3, 1)  # (B,C,H,W) -> (B,H,W,C)\n",
    "        save_image_grid(samples_np, f\"samples/epoch_{epoch + 1}.png\")\n",
    "\n",
    "# 绘制训练曲线\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.savefig('logs/training_curve.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b405e915-aceb-4666-8d61-aa008f9af30c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
